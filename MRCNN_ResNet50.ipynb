{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T09:41:48.946743Z",
     "start_time": "2020-08-06T09:41:45.978941Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from PIL import Image, ImageOps\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T13:36:01.118711Z",
     "start_time": "2020-07-29T13:36:00.940711Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    shutil.rmtree(os.getcwd()+'\\\\images')\n",
    "except:\n",
    "    None\n",
    "\n",
    "try:\n",
    "    shutil.rmtree(os.getcwd()+'\\\\images\\\\cropped')\n",
    "except:\n",
    "    None\n",
    "\n",
    "try:\n",
    "    os.makedirs(os.getcwd()+'\\\\images\\\\cropped\\\\class_1')\n",
    "    os.makedirs(os.getcwd()+'\\\\images\\\\cropped\\\\class_2')\n",
    "except:\n",
    "    None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T07:53:28.211962Z",
     "start_time": "2020-07-30T07:53:28.207963Z"
    }
   },
   "outputs": [],
   "source": [
    "valid_video_path = os.getcwd()+'\\\\videos_validation\\\\'\n",
    "img_path = os.getcwd()+'\\\\images\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capture frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T13:36:01.130717Z",
     "start_time": "2020-07-29T13:36:01.125713Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# vidcap = cv2.VideoCapture('video.mp4')\n",
    "\n",
    "# def getFrame(sec):\n",
    "#     vidcap.set(cv2.CAP_PROP_POS_MSEC,sec*1000)\n",
    "#     hasFrames,image = vidcap.read()\n",
    "#     if hasFrames:\n",
    "#         cv2.imwrite(img_path + str(count-1)+\".jpg\", image)     # save frame as JPG file\n",
    "#     return hasFrames\n",
    "# sec = 0\n",
    "# frameRate = 10 #//it will capture image in each 0.5 second\n",
    "# count=1\n",
    "# success = getFrame(sec)\n",
    "# while success:\n",
    "#     count = count + 1\n",
    "#     sec = sec + frameRate\n",
    "#     sec = round(sec, 2)\n",
    "#     success = getFrame(sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T13:36:01.141717Z",
     "start_time": "2020-07-29T13:36:01.132715Z"
    }
   },
   "outputs": [],
   "source": [
    "def vid_to_frames(path = None, vid_name = None, frate_sec = 30):\n",
    "    vidcap = cv2.VideoCapture(path)\n",
    "\n",
    "    img_path = os.getcwd()+'\\\\images\\\\'\n",
    "\n",
    "    def getFrame(sec):\n",
    "        vidcap.set(cv2.CAP_PROP_POS_MSEC,sec*1000)\n",
    "        hasFrames,image = vidcap.read()\n",
    "        if hasFrames:\n",
    "            cv2.imwrite(img_path + str(count-1)+ '_' + vid_name[:-4] + \".jpg\", image)     # save frame as JPG file\n",
    "        return hasFrames\n",
    "    sec = 0\n",
    "    frameRate = frate_sec\n",
    "    count=1\n",
    "    success = getFrame(sec)\n",
    "    while success:\n",
    "        count = count + 1\n",
    "        sec = sec + frameRate\n",
    "        sec = round(sec, 2)\n",
    "        success = getFrame(sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T13:36:56.188706Z",
     "start_time": "2020-07-29T13:36:01.143715Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in tqdm(os.listdir(valid_video_path)):\n",
    "    vid_to_frames(valid_video_path + i, i, 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T13:45:05.743064Z",
     "start_time": "2020-06-10T13:45:05.735063Z"
    }
   },
   "source": [
    "## Load R-CNN model (COCO, resnet50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T07:53:29.668513Z",
     "start_time": "2020-07-30T07:53:29.661607Z"
    },
    "code_folding": [
     5
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "# model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# model.eval()\n",
    "\n",
    "COCO_INSTANCE_CATEGORY_NAMES = [\n",
    "    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
    "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n",
    "    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
    "    'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A',\n",
    "    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
    "    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n",
    "    'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n",
    "    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
    "    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',\n",
    "    'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
    "    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book',\n",
    "    'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting prediction bbox, classes, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T07:53:30.978604Z",
     "start_time": "2020-07-30T07:53:30.968605Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_prediction(img_path, threshold, device):\n",
    "    img = Image.open(img_path) # Load the image\n",
    "    transform = transforms.Compose([transforms.ToTensor()]) # Defing PyTorch Transform\n",
    "    img = transform(img).to(device) # Apply the transform to the image\n",
    "    pred = model([img]) # Pass the image to the model\n",
    "    pred_class = [COCO_INSTANCE_CATEGORY_NAMES[i] for i in list(pred[0]['labels'].cpu().numpy())] # Get the Prediction Score\n",
    "    pred_boxes = [[i[0], i[1], i[2], i[3]] for i in list(pred[0]['boxes'].cpu().detach().numpy())] # Bounding boxes\n",
    "    pred_score = list(pred[0]['scores'].cpu().detach().numpy())\n",
    "    pred_mask = list(pred[0]['masks'].cpu().detach().numpy()) #MASK RCNN\n",
    "\n",
    "#     print(pred_boxes, pred_class, pred_score, pred_mask)\n",
    "#     print('^'*20)\n",
    "    \n",
    "    pred_t = [pred_score.index(x) for x in pred_score if x > threshold][-1] # Get list of index with score greater than threshold.\n",
    "    pred_boxes = pred_boxes[:pred_t+1]\n",
    "    pred_class = pred_class[:pred_t+1]\n",
    "    pred_score = pred_score[:pred_t+1]\n",
    "    pred_mask = pred_mask[:pred_t+1] #MASK RCNN\n",
    "    \n",
    "    return pred_boxes, pred_class, pred_score, pred_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T09:41:59.285054Z",
     "start_time": "2020-08-06T09:41:59.280067Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T09:41:59.654558Z",
     "start_time": "2020-08-06T09:41:59.460524Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = torch.load(os.getcwd() + '/models/' + 'MRCNN.pt', map_location=torch.device('cpu'))\n",
    "model.eval()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T07:53:33.903689Z",
     "start_time": "2020-07-30T07:53:33.896684Z"
    }
   },
   "outputs": [],
   "source": [
    "def mask(a,b,c,d, mask_param = 50):\n",
    "    if x != 'cropped':\n",
    "        img = Image.open(img_path + x)\n",
    "        suf = 0\n",
    "        idx = 0\n",
    "        \n",
    "        for per, box, msk in zip(b, a, d):\n",
    "            if per == 'person':\n",
    "\n",
    "                mm = (msk.squeeze() * 255).astype(np.uint8)\n",
    "                mm[mm<mask_param] = 0 #regulates area of the mask based on sharpness\n",
    "                mm[mm>0] = 255 #full white for all non-null mask pixels\n",
    "                mm = Image.fromarray(mm)\n",
    "                mm = ImageOps.invert(mm)\n",
    "\n",
    "                to_save = Image.composite(mm, img, mm)\n",
    "                to_save = to_save.crop(box=box)\n",
    "                \n",
    "                to_save.save(img_path + 'cropped\\\\class_1\\\\' + x + '_' + str(idx) + '_' + str(suf) + '.jpg')\n",
    "                \n",
    "                suf += 1\n",
    "                idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T07:55:53.713834Z",
     "start_time": "2020-07-30T07:53:37.041934Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/20 [00:00<?, ?it/s]C:\\Users\\glebs\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "..\\torch\\csrc\\utils\\python_arg_parser.cpp:756: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero(Tensor input, *, Tensor out)\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(Tensor input, *, bool as_tuple)\n",
      "100%|██████████████████████████████████████████| 20/20 [02:16<00:00,  6.83s/it]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "excl_list = []\n",
    "l = 3\n",
    "\n",
    "\n",
    "for x in tqdm(os.listdir(img_path)[:20]):\n",
    "    a, b, c, d = [0], [0], [0], [0]\n",
    "    try:\n",
    "        a, b, c, d = get_prediction(img_path+x, 0.90, device)\n",
    "        mask(a,b,c,d, mask_param = 50)\n",
    "    except:\n",
    "        print(x, ' _'*20)\n",
    "        excl_list.append(x)\n",
    "#     results.append([a,b,c,d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T06:47:12.284178Z",
     "start_time": "2020-07-29T06:46:55.411113Z"
    }
   },
   "outputs": [],
   "source": [
    "for idx, x in enumerate(os.listdir(img_path)):\n",
    "    if x != 'cropped':\n",
    "        img = Image.open(img_path + x)\n",
    "        suf = 0\n",
    "        for per, box, msk in zip(results[idx][1], results[idx][0], results[idx][3]):\n",
    "            if per == 'person':\n",
    "                \n",
    "                to_save = img.crop(box=box)\n",
    "                to_save.save(img_path + 'cropped\\\\class_2\\\\' + str(idx) + '_' + str(suf) + '.jpg')\n",
    "                suf += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T13:56:59.199860Z",
     "start_time": "2020-06-10T13:56:59.187884Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "### Check results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-27T16:49:30.063858Z",
     "start_time": "2020-07-27T16:49:29.879225Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# rand_img = random.choice(os.listdir(img_path))\n",
    "# sers = pd.Series(os.listdir(img_path))\n",
    "# idx = sers[sers == rand_img].index.values[0]\n",
    "\n",
    "# img = np.array(Image.open(img_path+rand_img))\n",
    "\n",
    "# for per, box in zip(results[idx][1], results[idx][0]):\n",
    "#     if per == 'person':\n",
    "#         img = cv2.rectangle(img, (box[0], box[1]), ((box[2], box[3])), (0,0,255), 10)\n",
    "\n",
    "# plt.figure(figsize=(7,7))\n",
    "# plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T18:52:14.994677Z",
     "start_time": "2020-06-22T18:52:14.924659Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# torch.save(model, 'C:\\\\Users\\\\Gleb\\\\Documents\\\\Data science\\\\PROJECTS\\\\P1_object_detection\\\\Video_learning\\\\box_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-12T17:06:43.561426Z",
     "start_time": "2020-07-12T17:06:43.557430Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# os.listdir(img_path).intersection(excl_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Create images for classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-27T16:11:59.419411Z",
     "start_time": "2020-07-27T16:11:59.414401Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# img=Image.open(img_path+rand_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-27T16:12:40.004685Z",
     "start_time": "2020-07-27T16:12:39.509158Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    shutil.rmtree(os.getcwd()+'\\\\images\\\\cropped')\n",
    "except:\n",
    "    None\n",
    "\n",
    "try:\n",
    "    os.mkdir(os.getcwd()+'\\\\images\\\\cropped\\\\')\n",
    "    os.mkdir(os.getcwd()+'\\\\images\\\\cropped\\\\class_1')\n",
    "except:\n",
    "    None\n",
    "\n",
    "for idx, x in enumerate(os.listdir(img_path)):\n",
    "    if x != 'cropped':\n",
    "        img = Image.open(img_path + x)\n",
    "        suf = 0\n",
    "        for per, box in zip(results[idx][1], results[idx][0]):\n",
    "            if per == 'person':\n",
    "                to_save = img.crop(box=box)\n",
    "                to_save.save(img_path + 'cropped\\\\class_1\\\\' + str(idx) + '_' + str(suf) + '.jpg')\n",
    "                suf += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-27T18:29:12.072954Z",
     "start_time": "2020-07-27T18:29:12.057952Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "shuffle = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-27T18:29:12.557100Z",
     "start_time": "2020-07-27T18:29:12.538096Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Data augmentation and normalization for training\n",
    "# Just normalization for testidation\n",
    "data_transforms = {\n",
    "    'cropped': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "#         transforms.CenterCrop(256),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "data_dir = os.getcwd()+'\\\\images\\\\'\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['cropped']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x],\n",
    "                                             shuffle=shuffle, num_workers=4)\n",
    "              for x in ['cropped']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['cropped']}\n",
    "class_names = image_datasets['cropped'].classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Random image show (from dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-27T18:29:15.345996Z",
     "start_time": "2020-07-27T18:29:13.994698Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def imshow(inp, title=None):\n",
    "    plt.figure(figsize=(7,7))\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "\n",
    "# Get a batch of training data\n",
    "inputs, classes = next(iter(dataloaders['cropped']))\n",
    "\n",
    "# Make a grid from batch\n",
    "out = torchvision.utils.make_grid(inputs)\n",
    "\n",
    "imshow(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Loading classification model weights (ResNet 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-27T18:29:17.038345Z",
     "start_time": "2020-07-27T18:29:17.035345Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# loaded_model = models.resnet50(pretrained=True)\n",
    "# num_ftrs = loaded_model.fc.in_features\n",
    "# # Here the size of each output sample is set to 2.\n",
    "# # Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
    "# loaded_model.fc = nn.Linear(num_ftrs, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-27T18:29:17.271028Z",
     "start_time": "2020-07-27T18:29:17.258025Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Path_model_wts = 'C:/Users/gleb/Documents/Data science/PROJECTS/P1_object_detection/model1.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-27T18:29:17.441780Z",
     "start_time": "2020-07-27T18:29:17.433779Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Path_model_wts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-27T18:29:17.877212Z",
     "start_time": "2020-07-27T18:29:17.650500Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "loaded_model = torch.load('C:/Users/gleb/Documents/Data science/PROJECTS/P1_object_detection/Video_learning/predict_model.pt')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-27T18:29:20.544250Z",
     "start_time": "2020-07-27T18:29:20.138374Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# loaded_model.load_state_dict(torch.load(Path_model_wts))\n",
    "loaded_model = torch.load('C:/Users/gleb/Documents/Data science/PROJECTS/P1_object_detection/model_full.pt')\n",
    "loaded_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-27T18:29:21.669517Z",
     "start_time": "2020-07-27T18:29:21.667517Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# torch.save(loaded_model, 'C:\\\\Users\\\\Gleb\\\\Documents\\\\Data science\\\\PROJECTS\\\\P1_object_detection\\\\Video_learning\\\\predict_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Visualization of results (probs + image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-27T18:30:54.003044Z",
     "start_time": "2020-07-27T18:30:53.987040Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "all_preds = []\n",
    "\n",
    "def visualize_model_cpu(model, data_set = 'test', num_images=6):\n",
    "    \n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(dataloaders[data_set]):\n",
    "            inputs = inputs.to(device)#.to('cpu')\n",
    "            labels = labels.to(device)#.to('cpu')\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            smth, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            title = torch.nn.functional.softmax(outputs, dim=1)\n",
    "            all_preds.append(preds.cpu().numpy()[0])\n",
    "            \n",
    "            for j in range(inputs.size()[0]):\n",
    "                images_so_far += 1\n",
    "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
    "                ax.axis('off')\n",
    "                ax.set_title('Gleb: {:.2f}, Katya: {:.2f}'.format(title.cpu().numpy()[0][0], title.cpu().numpy()[0][1]))\n",
    "                imshow(inputs.cpu().data[j])\n",
    "\n",
    "                if images_so_far == num_images:\n",
    "#                     model.train(mode=was_training)\n",
    "                    return\n",
    "    return all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-27T18:30:44.153517Z",
     "start_time": "2020-07-27T18:30:42.020040Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "visualize_model_cpu(loaded_model, 'cropped', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T18:52:22.256708Z",
     "start_time": "2020-06-22T18:52:22.250704Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "all_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-27T18:38:47.484416Z",
     "start_time": "2020-07-27T18:38:47.194873Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "torch.save(model, 'C:\\\\Users\\\\Gleb\\\\Documents\\\\Data science\\\\PROJECTS\\\\P1_object_detection\\\\Video_learning\\\\MRCNN.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "notify_time": "5",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
